{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "'''\n",
    "This file is used to retrive relative paragraghs from the article.\n",
    "Using TF-IDF weights, and inverted indexes to speed up the algorithm.\n",
    "'''\n",
    "\n",
    "# define the lemmatize function\n",
    "def lmz(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word = word.lower()\n",
    "    lemma = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "# get bag-of-words from tokenized document\n",
    "def get_BOW(text):\n",
    "    BOW = dict()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for word in text:\n",
    "        word_lemma = lmz(word)\n",
    "        if word_lemma in stop_words:\n",
    "            continue\n",
    "        BOW[word_lemma] = BOW.get(word_lemma,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def get_bigram(token_list):\n",
    "    result = list()\n",
    "    result.append('_' + token_list[0])\n",
    "    for i in range(len(token_list) - 1):\n",
    "        result.append(token_list[i] + '_' + token_list[i + 1])\n",
    "    result.append(token_list[-1] + '_')\n",
    "    return result + token_list\n",
    "\n",
    "# function for deriving count given sense\n",
    "def get_count(sense, origin_word):\n",
    "    for lemma in sense.lemmas():\n",
    "        if lemma.name() == origin_word:\n",
    "            return lemma.count()\n",
    "    return 0\n",
    "\n",
    "# function for deciding if a word is ambiguous or not\n",
    "def not_ambiguous(word):\n",
    "    syn_sets = wn.synsets(word)\n",
    "    if len(syn_sets) < 2:\n",
    "        return True\n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    for synset in syn_sets:\n",
    "        count = get_count(synset, word)\n",
    "        if count > count_2:\n",
    "            if count > count_1:\n",
    "                count_2 = count_1\n",
    "                count_1 = count\n",
    "            else:\n",
    "                count_2 = count\n",
    "    if count_1 >= count_2 * 5:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def main_process(fileName = 'testing.json' ,testing = False):\n",
    "    '''\n",
    "    load documents from json\n",
    "    [\n",
    "        {\n",
    "            docid: num\n",
    "            text: [\n",
    "                    para_1,\n",
    "                    para_2,\n",
    "                    ...\n",
    "                ]\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "    '''\n",
    "    doc = json.load(open('documents.json'))\n",
    "\n",
    "    # the bag-of-words representations for each article\n",
    "    doc_dict = [None] * len(doc)\n",
    "    for doc_item in doc:\n",
    "        para_dict = list()\n",
    "        for para in doc_item['text']:\n",
    "            # get the bigram set\n",
    "            bigram_list = get_bigram(nltk.word_tokenize(para))\n",
    "            para_dict.append(get_BOW(bigram_list))\n",
    "        doc_dict[doc_item['docid']] = para_dict\n",
    "\n",
    "    # For each document, get the TF-IDF weight, store all\n",
    "    # the information for later use\n",
    "    transformer_pair = [None] * len(doc_dict)\n",
    "    tf_idf_matrix_reg_doc = [None] * len(doc_dict)\n",
    "\n",
    "    for doc_index in range(len(doc_dict)):\n",
    "        vectorizer = DictVectorizer()\n",
    "        transformer = TfidfTransformer()\n",
    "\n",
    "        term_para_matrix = vectorizer.fit_transform(doc_dict[doc_index])\n",
    "        tf_idf_matrix_reg_doc[doc_index] = transformer.fit_transform(term_para_matrix)\n",
    "        transformer_pair[doc_index] = (vectorizer, transformer)\n",
    "\n",
    "    if testing:\n",
    "        train = json.load(open('training.json'))\n",
    "        success = 0\n",
    "        length = 0\n",
    "        for train_item in train:\n",
    "            bigram_list = get_bigram(nltk.word_tokenize(train_item['question']))\n",
    "            question = get_BOW(bigram_list)\n",
    "            vectorizer, transformer = transformer_pair[train_item['docid']]\n",
    "            score = [0] * len(doc_dict[train_item['docid']])\n",
    "            word_feature_map = vectorizer.vocabulary_\n",
    "            for word_from_question in question:\n",
    "                flag = True\n",
    "                if not not_ambiguous(word_from_question):\n",
    "                    flag = False\n",
    "                for index in range(len(doc_dict[train_item['docid']])):\n",
    "                    if word_from_question in doc_dict[train_item['docid']][index]:\n",
    "                        if flag:\n",
    "                            score[index] += (tf_idf_matrix_reg_doc[train_item['docid']][index, word_feature_map[word_from_question]])\n",
    "                        else:\n",
    "                            score[index] += (tf_idf_matrix_reg_doc[train_item['docid']][index, word_feature_map[word_from_question]] * 2.0 / 1.6)\n",
    "            if train_item['answer_paragraph'] in np.argsort(score)[-6:]:\n",
    "                success += 1\n",
    "            length += 1\n",
    "        print('The retrival accuracy is', success * 1.0 / float(length))\n",
    "    else:\n",
    "        items = json.load(open(fileName))\n",
    "        related_para = [None] * len(items)\n",
    "\n",
    "        for item_index, item in enumerate(items):\n",
    "            question = get_BOW(nltk.word_tokenize(item['question']))\n",
    "            vectorizer, transformer = transformer_pair[item['docid']]\n",
    "            score = [0] * len(doc_dict[item['docid']])\n",
    "            word_feature_map = vectorizer.vocabulary_\n",
    "            for word_from_question in question:\n",
    "                for index in range(len(doc_dict[item['docid']])):\n",
    "                    if word_from_question in doc_dict[item['docid']][index]:\n",
    "                        score[index] += tf_idf_matrix_reg_doc[item['docid']][index, word_feature_map[word_from_question]]\n",
    "            related_para[item_index] = np.argsort(score)[-5:]\n",
    "        return related_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The retrival accuracy is', 0.9352221120818829)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(main_process(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "related_para_train = main_process('training.json', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  1  5 23 22]\n"
     ]
    }
   ],
   "source": [
    "print(related_para_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46476\n"
     ]
    }
   ],
   "source": [
    "doc = json.load(open('documents.json'))\n",
    "train = json.load(open('training.json'))\n",
    "dev = json.load(open('devel.json'))\n",
    "test = json.load(open('testing.json'))\n",
    "\n",
    "# get all the questions\n",
    "all_questions = list()\n",
    "for collection in [train, dev]:\n",
    "    for item in collection:\n",
    "        all_questions.append([lmz(token) for token in nltk.word_tokenize(item['question'])])\n",
    "        \n",
    "print len(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18875\n"
     ]
    }
   ],
   "source": [
    "# get all the documents\n",
    "all_para = list()\n",
    "for item in doc:\n",
    "    for para in item['text']:\n",
    "        all_para.append([lmz(token) for token in nltk.word_tokenize(para)])\n",
    "        \n",
    "print len(all_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766 60\n"
     ]
    }
   ],
   "source": [
    "para_max_len = np.max([len(para) for para in all_para])\n",
    "question_max_len = np.max([len(question) for question in all_questions])\n",
    "\n",
    "print para_max_len, question_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for para in all_para:\n",
    "    para += (['<\\s>'] * (para_max_len - len(para)))\n",
    "    \n",
    "for question in all_questions:\n",
    "    question += (['<\\s>'] * (question_max_len - len(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(all_para + all_questions, size = 300, min_count = 1, workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "light\n",
      "quantum\n"
     ]
    }
   ],
   "source": [
    "a = [lmz(token) for token in nltk.word_tokenize(doc[0]['text'][0])]\n",
    "for i in range(87,90):\n",
    "    print a[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
